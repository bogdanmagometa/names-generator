{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement names generator\n",
    "\n",
    "Use architecture from Bengio et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = set(sum((list(s) for s in words), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols = all_letters.union({'.'})\n",
    "stoi = {'.': 0}\n",
    "stoi.update({letter: idx for idx, letter in enumerate(all_letters, 1)})\n",
    "\n",
    "# create dataset\n",
    "X = list()\n",
    "y = list()\n",
    "for word in words:\n",
    "    word = block_size * '.' + word + '.'\n",
    "    for idx in range(block_size, len(word)):\n",
    "        prev = word[(idx-block_size):idx]\n",
    "        X.append(list(stoi[s] for s in prev))\n",
    "        y.append(stoi[word[idx]])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors of parameters\n",
    "C = torch.randn((len(all_symbols), embeddings_len)) # matrix of embeddings\n",
    "C.requires_grad = True\n",
    "\n",
    "biases = torch.randn(len(all_symbols))\n",
    "biases.requires_grad = True\n",
    "\n",
    "weights = torch.randn((block_size * embeddings_len, len(all_symbols)))\n",
    "weights.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X: torch.Tensor):\n",
    "    c = C[X]\n",
    "\n",
    "    first_layer_volume = c.view(X.shape[0], -1)\n",
    "    first_layer_activations = torch.tanh(first_layer_volume)\n",
    "\n",
    "    second_layer_volume = (first_layer_activations @ weights) + biases\n",
    "    second_layer_activations = torch.softmax(second_layer_volume, 1)\n",
    "\n",
    "    return second_layer_activations\n",
    "\n",
    "def create_loss(predictions, actual):\n",
    "    return - predictions[range(len(predictions)), actual].log().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4654412269592285\n",
      "2.482477903366089\n",
      "2.2996668815612793\n",
      "2.384448766708374\n",
      "2.2171342372894287\n",
      "2.2757508754730225\n",
      "2.451603651046753\n",
      "2.210165023803711\n",
      "2.3788421154022217\n",
      "2.2792611122131348\n"
     ]
    }
   ],
   "source": [
    "num_iters = 100_000\n",
    "lr = 0.001\n",
    "\n",
    "for num_iter in range(num_iters):\n",
    "    # get batch\n",
    "    indices = torch.randint(0, len(X) - 1, (32,))\n",
    "    batch_X = X[indices]\n",
    "    batch_y = y[indices]\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = create_loss(create_model(batch_X), batch_y)\n",
    "\n",
    "    # dispay loss\n",
    "    if (num_iter % 10_000) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    # zerograd\n",
    "    C.grad = None\n",
    "    biases.grad = None\n",
    "    weights.grad = None\n",
    "\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    C.data -= lr * C.grad\n",
    "    biases.data -= lr * biases.grad\n",
    "    weights.data -= lr * weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2974960803985596"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_loss(create_model(X), y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
